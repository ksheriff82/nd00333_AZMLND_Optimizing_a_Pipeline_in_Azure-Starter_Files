# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**

This dataset contains information about the customers, for which we need to generate a model provide a loan eligibility of yes/no based on the data.
**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
Generated two models using hyper drive and AutoML
Hyper drive Model:
Used Scikit-learn Pipeline to test the best combination of parameters for a SKLearn logistic regression model. the best hyper drive model we got had accuracy of 0.90492368. We also used the BanditPolicy for early termination to avoid excessive usage of resources in case the algorithm being evaluated is not having the desired results while processing, this would avoid wastage of resources by failing fast.

AutoML:Used different regression models, evaluated by AUC_weighted. The best AutoML model was VotingEnsemble with  AUC_weighted of 0.9486.
## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The Scikit-Learn Pipeline imported a TabularDataset of clean data and used logistic regression for the binary outcome variable. Used HyperDrive to automate the testing of two parameters: --C and --max_iter. --C is the inverse of regularization strength, and max_iter is maximum number of iterations to converge. A choice of three different values were tested for each parameter.

![image](https://github.com/ksheriff82/nd00333_AZMLND_Optimizing_a_Pipeline_in_Azure-Starter_Files/assets/43680905/91cff241-0ce5-4b39-80ea-6a8e45dd55ce)

**What are the benefits of the parameter sampler you chose?**
A Random parameter sampler was used as it provides choice options for the sampling method.

**What are the benefits of the early stopping policy you chose?**
 BanditPolicy keeps the program from running too long and taking too many resources.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
The AutoML tested 30 different regression algorithms. The best option for optimizing AUC_weighted was VotingEnsemble.
we used the Auto ml with the following parameters
task= 'classification',
primary_metric= 'AUC_weighted',
n_cross_validations=2

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
The difference between two models in quite close . Hyper driver has accuracy of 0.90492368, where as AutoML AUC_Weighted metric is 0.9486.
The hyper drive had lesser number of runs, where as autoML was able to run several combinations using various algorithms with minimal config tuning.
Hyper drive ran quickly and ran only for 7 mins , where as AutoML took about 30 mins.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
We could use this knowledge of running AutoML for generating various models , that could be helpful in predicting the system behavior based on historic data.
We can try to update the primary metric and n_cross_validations in order to get a better models.


